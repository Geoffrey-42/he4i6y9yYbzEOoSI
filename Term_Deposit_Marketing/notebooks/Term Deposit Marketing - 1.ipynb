{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3933908b",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b269cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import isclose\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rd\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from imblearn.under_sampling import CondensedNearestNeighbour, RandomUnderSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e9e200",
   "metadata": {},
   "source": [
    "Importing raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31345143",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dir = 'C:/Users/GEOFF/OneDrive/Documents/Apziva/Term_Deposit_Marketing/'\n",
    "path = dir+'data/raw/'\n",
    "dataframe = pd.read_csv(path+\"term-deposit-marketing-2020.csv\")\n",
    "target_name = 'y'\n",
    "target = dataframe[target_name]\n",
    "data = dataframe.drop(columns = [target_name])\n",
    "\n",
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "\n",
    "numerical_columns = numerical_columns_selector(data)\n",
    "categorical_columns = categorical_columns_selector(data)\n",
    "\n",
    "print('There are %s costumer datapoints'%len(data))\n",
    "print('\\nThere are %s features, which are:'%len(data.columns))\n",
    "print(set(data.columns))\n",
    "\n",
    "print('\\nThe numerical features are:')\n",
    "print(numerical_columns)\n",
    "\n",
    "print('\\nThe categorical features are:')\n",
    "print(categorical_columns)\n",
    "\n",
    "print('\\nThe 5 first customers data are:')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f09116",
   "metadata": {},
   "source": [
    "Discriminating the heterogeneous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e0c680",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nDescription of the numerical features in the data:')\n",
    "print(data.describe())\n",
    "non_binary_categorical_columns = {}\n",
    "binary_categorical_columns = []\n",
    "for category in categorical_columns:\n",
    "    nb_cat = pd.Series.nunique(data[category])\n",
    "    if nb_cat > 2:\n",
    "        non_binary_categorical_columns[category] = f' ({nb_cat} values found)'\n",
    "    else:\n",
    "        binary_categorical_columns.append(category)\n",
    "\n",
    "print('\\nThe binary categorical columns are:')\n",
    "for cat in binary_categorical_columns:\n",
    "    print(cat, ' (2 values found)')\n",
    "print('\\nThe other categorical columns are:')\n",
    "for key, value in non_binary_categorical_columns.items():\n",
    "    print(key, value)\n",
    "non_binary_categorical_columns = list(non_binary_categorical_columns.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8b16ae",
   "metadata": {},
   "source": [
    "Encoding the categorical features and scaling the numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f710db14",
   "metadata": {},
   "outputs": [],
   "source": [
    "ColumnTransformer_ = make_column_transformer(\n",
    "    (StandardScaler(), numerical_columns),\n",
    "    (OneHotEncoder(), categorical_columns),\n",
    "    remainder='passthrough',\n",
    "    n_jobs = 4)\n",
    "\n",
    "Transformed_Columns = ColumnTransformer_.fit_transform(data)\n",
    "columns = ColumnTransformer_.get_feature_names_out()\n",
    "print(f'The transformed features names are\\n{columns}\\n')\n",
    "transformed_df = pd.DataFrame(Transformed_Columns.toarray(), columns = columns)\n",
    "\n",
    "# Dropping the \"no\" columns resulting from the one-hot encoding of binary categorical features\n",
    "columns_to_drop = []\n",
    "for feature in binary_categorical_columns:\n",
    "    columns_to_drop.append('onehotencoder__'+feature+'_no')\n",
    "transformed_df.drop(columns = columns_to_drop, inplace = True)\n",
    "\n",
    "print(f'The transformed dataset has a shape of {transformed_df.shape}')\n",
    "print(f'A total of {len(columns)-transformed_df.shape[1]} useless encoded features were removed')\n",
    "\n",
    "transformed_target = LabelEncoder().fit_transform(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6180fe",
   "metadata": {},
   "source": [
    "Undersampling the data to obtain balance regarding to the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa81dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The data is unbalanced')\n",
    "print(f'The proportion of the minority class is {sum(transformed_target)/transformed_target.shape[0]:.2f}\\n')\n",
    "\n",
    "# undersample = CondensedNearestNeighbour(n_neighbors=1)\n",
    "undersample = RandomUnderSampler()\n",
    "X, y = undersample.fit_resample(transformed_df, transformed_target)\n",
    "\n",
    "print('The dataset resulting from undersampling has a shape of', X.shape, \n",
    "      '\\nThe label array resulting from undersampling has a shape of', y.shape)\n",
    "print(f'The proportion of the minority class is now {sum(y)/y.shape[0]:.2f}\\n')\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    np.array(X), y, test_size = 0.2, stratify = y, random_state=42)\n",
    "\n",
    "print('The data resulting from undersampling is now divided into a train and a test set 80/20%')\n",
    "print('The train set and label array have a shape of ', data_train.shape, target_train.shape)\n",
    "print('The test set and label array have a shape of ', data_test.shape, target_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944543b7",
   "metadata": {},
   "source": [
    "Trying a Decision Tree model with a depth of 1 to detect the most informative feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f35574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_tree_model = DecisionTreeClassifier(max_depth = 1,\n",
    "                                    max_leaf_nodes = 2,\n",
    "                                    max_features = None,\n",
    "                                    random_state = 42)\n",
    "\n",
    "simple_tree_cv_results = cross_validate(simple_tree_model, data_train, target_train, cv=5, \n",
    "                                        return_train_score = True)\n",
    "\n",
    "t_scores = simple_tree_cv_results[\"train_score\"]\n",
    "print(\"The mean cross-validation training accuracy is: \"\n",
    "      f\"{t_scores.mean():.3f} ± {t_scores.std():.3f}\")\n",
    "scores = simple_tree_cv_results[\"test_score\"]\n",
    "print(\"The mean cross-validation accuracy is: \"\n",
    "      f\"{scores.mean():.3f} ± {scores.std():.3f}\")\n",
    "\n",
    "simple_tree_model.fit(data_train, target_train)\n",
    "plot_tree(simple_tree_model)\n",
    "\n",
    "print(f'\\nThe feature that yields the most information gain is {columns[3]}')\n",
    "\n",
    "this_transformed_column = transformed_df[columns[3]]\n",
    "this_column = np.array(dataframe['duration'])\n",
    "scaler = StandardScaler()\n",
    "this_retransformed_column = scaler.fit_transform(this_column.reshape(-1,1))\n",
    "print(f'The average call duration is {int(scaler.mean_)} sec\\n',\n",
    "      f'The standard deviation is {int(scaler.var_**0.5)} sec')\n",
    "\n",
    "threshold = scaler.inverse_transform(np.array([0.469]*40000).reshape(-1,1))\n",
    "print(f'The call duration threshold is {threshold[0,0]:.0f} sec')\n",
    "print(f'The mean cross-validation accuracy was {scores.mean():.3f} just from using that criteria')\n",
    "\n",
    "# Checking if the scaler StandardScaler fitted the column with the same parameters as ColumnTranformer did\n",
    "equal = np.array([isclose(this_retransformed_column[i], this_transformed_column[i], abs_tol = 1e-6) for i in range(40000)])\n",
    "assert equal.all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5a6dcf",
   "metadata": {},
   "source": [
    "Trying a Logistic Regression model (works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37832e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "logis_model = LogisticRegression(penalty = 'l2',\n",
    "                                 C = 1.0,\n",
    "                                 solver = 'newton-cholesky', # sag and saga solvers can be tried as well\n",
    "                                 max_iter = 100,\n",
    "                                 n_jobs = 4)\n",
    "\n",
    "logis_cv_results = cross_validate(logis_model, data_train, target_train, cv=5, \n",
    "                                  return_train_score = True)\n",
    "\n",
    "t_scores = logis_cv_results[\"train_score\"]\n",
    "print(\"The mean cross-validation training accuracy is: \"\n",
    "      f\"{t_scores.mean():.3f} ± {t_scores.std():.3f}\")\n",
    "scores = logis_cv_results[\"test_score\"]\n",
    "print(\"The mean cross-validation accuracy is: \"\n",
    "      f\"{scores.mean():.3f} ± {scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b249de1d",
   "metadata": {},
   "source": [
    "Trying a shallow Decision Tree model (works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143502b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model = DecisionTreeClassifier(max_depth = 3,\n",
    "                                    max_leaf_nodes = 20,\n",
    "                                    max_features = None,\n",
    "                                    random_state = 42)\n",
    "\n",
    "tree_cv_results = cross_validate(tree_model, data_train, target_train, cv=5, \n",
    "                                  return_train_score = True)\n",
    "\n",
    "t_scores = tree_cv_results[\"train_score\"]\n",
    "print(\"The mean cross-validation training accuracy is: \"\n",
    "      f\"{t_scores.mean():.3f} ± {t_scores.std():.3f}\")\n",
    "scores = tree_cv_results[\"test_score\"]\n",
    "print(\"The mean cross-validation accuracy is: \"\n",
    "      f\"{scores.mean():.3f} ± {scores.std():.3f}\")\n",
    "tree_model.fit(data_train, target_train)\n",
    "plot_tree(tree_model)\n",
    "print(f'The selected features in this tree were:\\n{list(transformed_df.iloc[:,[3,30,37,25,29]].columns)}')\n",
    "print('From the decision tree, for clients with a call duration below the threshold\\n' \n",
    "       'if the last call was in April and if the client has no housing loan, or if last call was in March\\n'\n",
    "        'y is more likely to be 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cadd8d9",
   "metadata": {},
   "source": [
    "Trying a XGBoost model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "661773f9",
   "metadata": {},
   "source": [
    "The cell below takes a few minutes to run and can be skipped (GridSearch on XGBoost model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bfa9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier()\n",
    "parameters = {'n_estimators':list(range(1,501,100)), \n",
    "              'max_depth':list(range(1,11,1)), \n",
    "              'learning_rate':[round(10**(i/10),3) for i in range(-20, 0, 5)]}\n",
    "clf = GridSearchCV(estimator = model, param_grid = parameters, cv = 5)\n",
    "clf.fit(data_train, target_train)\n",
    "print('The best parameters in the specified grid are found to be\\n', clf.best_params_)\n",
    "print(f'With the model accuracy reaching {clf.score(data_train, target_train):0.3f} on the training set')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d048c577",
   "metadata": {},
   "source": [
    "Returns:\n",
    "The best parameters in the specified grid are found to be:\n",
    " {'learning_rate': 0.032, 'max_depth': 8, 'n_estimators': 401}\n",
    "\n",
    "With the model accuracy reaching 0.975 on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c841b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'learning_rate': 0.032, 'max_depth': 8, 'n_estimators': 400}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2727a7a6",
   "metadata": {},
   "source": [
    "Trying the XGBoost model with above found parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb1503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = best_params['n_estimators']\n",
    "max_depth = best_params['max_depth']\n",
    "learning_rate = best_params['learning_rate']\n",
    "\n",
    "model = XGBClassifier(n_estimators = n_estimators, \n",
    "                             max_depth = max_depth, \n",
    "                             learning_rate = learning_rate, \n",
    "                             random_state = 42)\n",
    "\n",
    "cv_results = cross_validate(model, data_train, target_train, cv=5,\n",
    "                            return_train_score = True)\n",
    "\n",
    "t_scores = cv_results[\"train_score\"]\n",
    "print(\"The mean training accuracy is: \"\n",
    "      f\"{t_scores.mean():.3f} ± {t_scores.std():.3f}\")\n",
    "scores = cv_results[\"test_score\"]\n",
    "print(\"The mean cross-validation accuracy is: \"\n",
    "      f\"{scores.mean():.3f} ± {scores.std():.3f}\")\n",
    "\n",
    "print('The model seems to be overfitting.\\n',\n",
    "     'An overfitting test will be run by changing the variable max_depth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3978a80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracies = []\n",
    "validation_accuracies = []\n",
    "for max_depth in range(1,11):\n",
    "    model = XGBClassifier(n_estimators = n_estimators, \n",
    "                                 max_depth = max_depth, \n",
    "                                 learning_rate = learning_rate, \n",
    "                                 random_state = 42)\n",
    "\n",
    "    cv_results = cross_validate(model, data_train, target_train, cv=5,\n",
    "                                return_train_score = True)\n",
    "\n",
    "    t_scores = cv_results[\"train_score\"]\n",
    "    training_accuracies.append(t_scores.mean())\n",
    "    scores = cv_results[\"test_score\"]\n",
    "    validation_accuracies.append(scores.mean())\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.plot(list(range(1,11)), training_accuracies, color = 'blue', label = 'Training Accuracy')\n",
    "ax.plot(list(range(1,11)), validation_accuracies, color = 'red', label = 'Validation Accuracy')\n",
    "ax.legend(fontsize = 12)\n",
    "ax.set_xlabel('Max_depth')\n",
    "ax.set_ylabel('Model Accuracy')\n",
    "ax.set_title('Overfitting test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2c6011",
   "metadata": {},
   "source": [
    "From the figure above, it can be seen that the cross-validation accuracy \n",
    "starts plateau-ing when max_depth >= 4.\n",
    "The XGBoost model with a max depth of 4 will be kept.\n",
    "The cross-validation results for each fold are provided below with more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0acd5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'learning_rate': 0.032, 'max_depth': 4, 'n_estimators': 400}\n",
    "n_estimators = best_params['n_estimators']\n",
    "max_depth = best_params['max_depth']\n",
    "learning_rate = best_params['learning_rate']\n",
    "\n",
    "model = XGBClassifier(n_estimators = n_estimators, \n",
    "                             max_depth = max_depth, \n",
    "                             learning_rate = learning_rate, \n",
    "                             random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be97ad3f",
   "metadata": {},
   "source": [
    "The cross-validation accuracy results will be displayed in detail for each run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34adca74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kfold_crossvalidation(partition):\n",
    "    accuracy = 0\n",
    "    n = 0\n",
    "    for train, test in partition:\n",
    "        model.fit(data_train[train], target_train[train])\n",
    "        target_train_predict = model.predict(data_train[test])\n",
    "        accuracy += model.score(data_train[test], target_train[test])\n",
    "        n += 1\n",
    "        print(classification_report(target_train[test], target_train_predict))\n",
    "    accuracy *= 1/n\n",
    "    print(f'cross validation accuracy = {accuracy}')\n",
    "\n",
    "skf = StratifiedKFold(5, shuffle = False)\n",
    "Kfold_crossvalidation(skf.split(data_train, target_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08320408",
   "metadata": {},
   "source": [
    "Next, trying to identify a costumer segment more likely to be successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9585e17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "segment_df = transformed_df.iloc[:,5:23]\n",
    "print(f'The features considered are now\\n{list(segment_df.columns)}')\n",
    "segment_data_train = data_train[:,5:23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bf03a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logis_model = LogisticRegression(penalty = 'l2',\n",
    "                                 C = 1.0,\n",
    "                                 solver = 'newton-cholesky', # sag and saga solvers can be tried as well\n",
    "                                 max_iter = 100,\n",
    "                                 n_jobs = 4)\n",
    "\n",
    "logis_cv_results = cross_validate(logis_model, segment_data_train, target_train, cv=5, \n",
    "                                  return_train_score = True)\n",
    "\n",
    "t_scores = logis_cv_results[\"train_score\"]\n",
    "print(\"The mean cross-validation training accuracy is: \"\n",
    "      f\"{t_scores.mean():.3f} ± {t_scores.std():.3f}\")\n",
    "scores = logis_cv_results[\"test_score\"]\n",
    "print(\"The mean cross-validation accuracy is: \"\n",
    "      f\"{scores.mean():.3f} ± {scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb61fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_tree_model = DecisionTreeClassifier(max_depth = None,\n",
    "                                            max_features = None,\n",
    "                                            random_state = 42)\n",
    "\n",
    "segment_tree_cv_results = cross_validate(segment_tree_model, segment_data_train, target_train, cv=5, \n",
    "                                         return_train_score = True)\n",
    "\n",
    "t_scores = segment_tree_cv_results[\"train_score\"]\n",
    "print(\"The mean cross-validation training accuracy is: \"\n",
    "      f\"{t_scores.mean():.3f} ± {t_scores.std():.3f}\")\n",
    "scores = segment_tree_cv_results[\"test_score\"]\n",
    "print(\"The mean cross-validation accuracy is: \"\n",
    "      f\"{scores.mean():.3f} ± {scores.std():.3f}\")\n",
    "print('\\nThe segment of costumers does not provide enough information to yield a conclusion on the target y.')\n",
    "print('No costumer segment should be prioritized')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed39605",
   "metadata": {},
   "source": [
    "Testing the previous models on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0246f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logis_model.fit(data_train, target_train)\n",
    "target_test_predict = logis_model.predict(data_test)\n",
    "print('\\nFor the Logistic Regression model, the test results are:\\n')\n",
    "print(classification_report(target_test, target_test_predict))\n",
    "\n",
    "tree_model.fit(data_train, target_train)\n",
    "target_test_predict = tree_model.predict(data_test)\n",
    "print('For the Decision Tree model, the test results are:\\n')\n",
    "print(classification_report(target_test, target_test_predict))\n",
    "\n",
    "model.fit(data_train, target_train)\n",
    "target_test_predict = model.predict(data_test)\n",
    "print('For the XGBoost model, the test results are:\\n')\n",
    "print(classification_report(target_test, target_test_predict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
